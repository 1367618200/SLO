import os
from typing import List, Optional, Sequence, Union

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from mmengine.evaluator import BaseMetric, DumpResults
from mmengine.fileio import dump
from mmengine.logging import print_log
from sklearn.metrics import cohen_kappa_score

from mmcls.evaluation.metrics.single_label import to_tensor
from mmcls.registry import METRICS
from mmcls.evaluation.metrics.single_label import Accuracy

from .multi_task import MultiTasksMetric  #NEW

# Based on: mmcls.evaluation.metrics.single_label  SingleLabelMetric()
@METRICS.register_module()
class Kappa(BaseMetric):
    """Kappa

    Args:
        collect_device (str): Device name used for collecting results from
            different ranks during distributed training. Must be 'cpu' or
            'gpu'. Defaults to 'cpu'.
        prefix (str, optional): The prefix that will be added in the metric
            names to disambiguate homonymous metrics of different evaluators.
            If prefix is not provided in the argument, self.default_prefix
            will be used instead. Defaults to None.
    """
    default_prefix: Optional[str] = ''

    def __init__(self,
                 num_classes: Optional[int] = None,
                 collect_device: str = 'cpu',
                 prefix: Optional[str] = None) -> None:
        super().__init__(collect_device=collect_device, prefix=prefix)
        self.num_classes = num_classes

    def process(self, data_batch, data_samples: Sequence[dict]):
        """Process one batch of data samples.

        The processed results should be stored in ``self.results``, which will
        be used to computed the metrics when all batches have been processed.

        Args:
            data_batch: A batch of data from the dataloader.
            data_samples (Sequence[dict]): A batch of outputs from the model.
        """

        for data_sample in data_samples:
            result = dict()
            pred_label = data_sample['pred_label']
            gt_label = data_sample['gt_label']
            if 'score' in pred_label:
                result['pred_score'] = pred_label['score'].cpu()
            else:
                num_classes = self.num_classes or data_sample.get(
                    'num_classes')
                # assert num_classes is not None, \
                #     'The `num_classes` must be specified if `pred_label` has ' \
                #     'only `label`.'
                result['pred_label'] = pred_label['label'].cpu()
                result['num_classes'] = num_classes
            result['gt_label'] = gt_label['label'].cpu()
            # NEW
            if result['gt_label'] == -100:
                continue
            # END NEW
            # Save the result to `self.results`.
            self.results.append(result)

    def compute_metrics(self, results: List):
        """Compute the metrics from processed results.

        Args:
            results (list): The processed results of each batch.

        Returns:
            Dict: The computed metrics. The keys are the names of the metrics,
            and the values are corresponding results.
        """
        # NOTICE: don't access `self.results` from the method. `self.results`
        # are a list of results from multiple batch, while the input `results`
        # are the collected results.
        metrics = {}

        # concat
        target = torch.cat([res['gt_label'] for res in results])
        if 'pred_score' in results[0]:
            pred = torch.stack([res['pred_score'] for res in results])
        else:
            # If only label in the `pred_label`.
            pred = torch.cat([res['pred_label'] for res in results])

        kappa = self.calculate(pred, target)
        metrics['kappa'] = kappa
        return metrics

    @staticmethod
    def calculate(
            pred: Union[torch.Tensor, np.ndarray, Sequence],
            target: Union[torch.Tensor, np.ndarray, Sequence],
    ) -> float:
        """Calculate the precision, recall, f1-score and support.

        Args:
            pred (torch.Tensor | np.ndarray | Sequence): The prediction
                results. It can be labels (N, ), or scores of every
                class (N, C).
            target (torch.Tensor | np.ndarray | Sequence): The target of
                each prediction with shape (N, ).

        Returns:
            - float: 100. * kappa
        """

        pred = to_tensor(pred)
        target = to_tensor(target).to(torch.int64)
        assert pred.size(0) == target.size(0), \
            f"The size of pred ({pred.size(0)}) doesn't match " \
            f'the target ({target.size(0)}).'

        if pred.ndim == 1:
            pred_label = pred.to(torch.int64).tolist()
            target = target.tolist()
        else:
            pred_score, pred_label = torch.topk(pred, k=1)
            pred_score = pred_score.flatten().tolist()
            pred_label = pred_label.flatten().tolist()
            target = target.flatten().tolist()

        kappa = cohen_kappa_score(target, pred_label, weights='quadratic')
        return 100. * kappa


# Based on: mmengine.evaluator.DumpResults
@METRICS.register_module()
class DumpCSVResults(DumpResults):
    default_prefix: Optional[str] = ''

    def __init__(self,
                 out_file_path,
                 csv_title=None,
                 is_dump_pkl=True,
                 collect_device='cpu') -> None:
        super(DumpResults, self).__init__(collect_device=collect_device)
        if not out_file_path.endswith(('.csv',)):
            raise ValueError('The output file must be a csv file.')
        self.csv_title = csv_title
        self.is_dump_pkl = is_dump_pkl
        self.out_file_path = out_file_path

    def compute_metrics(self, results: list) -> dict:
        """dump the prediction results to a pickle file."""

        if self.is_dump_pkl:
            out_pkl_path = self.out_file_path.replace('.csv', '.pkl')
            dump(results, out_pkl_path)
            print_log(f'Results has been saved to {out_pkl_path}.', logger='current')

        csv_data = []
        for x in results:
            name = os.path.basename(x['img_path']).split('.')[0]
            pred_label = x['pred_label']['label']  # tensor(int)  Shape: torch.Size([1])
            pred_score = x['pred_label']['score']  # tensor(float,)  Shape: torch.Size([num_classes])
            one_hot_label = F.one_hot(pred_label, num_classes=len(pred_score))
            one_hot_label = one_hot_label.flatten().tolist()
            csv_data.append([name] + one_hot_label)

        df = pd.DataFrame(csv_data, columns=self.csv_title)
        df.to_csv(self.out_file_path, index=False)
        print_log(f'Results has been saved to {self.out_file_path}.', logger='current')
        return {}

@METRICS.register_module()
class Accuracy_V1(Accuracy):
    def process(self, data_batch, data_samples: Sequence[dict]):
        for data_sample in data_samples:
            result = dict()
            pred_label = data_sample['pred_label']
            gt_label = data_sample['gt_label']

            if 'score' in pred_label:
                result['pred_score'] = pred_label['score'].cpu()
            else:
                result['pred_label'] = pred_label['label'].cpu()
            result['gt_label'] = gt_label['label'].cpu()

            # NEW
            if result['gt_label'] == -100:
                continue
            # END NEW

            # Save the result to `self.results`.
            self.results.append(result)

@METRICS.register_module()
class FundusMultiTasksMetric(MultiTasksMetric):
    def __init__(self,
                 value_format='{:5.2f}',
                 task_format='precision[{}] recall[{}] f-score[{}] kappa[{}]',
                 is_reverse_results=True,
                 *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.value_format = value_format
        self.task_format = task_format
        self.is_reverse_results = is_reverse_results

    def format_results(self, results):
        precision = results.get('single-label/precision_classwise', None)
        recall = results.get('single-label/recall_classwise', None)
        f1_score = results.get('single-label/f1-score_classwise', None)
        kappa = results.get('kappa', None)

        precision = [self.value_format.format(i) for i in precision]
        recall = [self.value_format.format(i) for i in recall]
        f1_score = [self.value_format.format(i) for i in f1_score]
        kappa = self.value_format.format(kappa)

        if self.is_reverse_results:
            precision.reverse()
            recall.reverse()
            f1_score.reverse()

        precision = ', '.join(precision)
        recall = ', '.join(recall)
        f1_score = ', '.join(f1_score)

        metric_str = self.task_format.format(precision, recall, f1_score, kappa)
        return metric_str

    def evaluate(self, size):
        metrics = {}
        for task_name in self._metrics:
            all_results = {}
            for metric in self._metrics[task_name]:
                name = metric.__class__.__name__
                if name == 'MultiTasksMetric' or metric.results:
                    results = metric.evaluate(size)
                else:
                    results = {metric.__class__.__name__: 0}
                all_results.update(results)
            result_str = self.format_results(all_results)
            metrics[task_name] = result_str
        return metrics